{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZVssIFHzYEv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697260815758,"user_tz":-360,"elapsed":6407,"user":{"displayName":"Zaed Khan","userId":"11956878547931761934"}},"outputId":"33678761-2e4a-48d5-cf2f-9c869423e185"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 78499836.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 72104579.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 71913936.89it/s]"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 19823651.16it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n"]}],"source":["# Importing necessary libraries\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import norm\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)\n","# Load MNIST dataset\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","\n","trainset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n","\n","testset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"araN8gaJ7UNY"},"source":["# Custom MLP - without dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qE2AfziozpPf"},"outputs":[],"source":["class Perceptron(nn.Module):\n","  def __init__(self, num_inputs):\n","    super(Perceptron, self).__init__()\n","    self.weights = nn.Parameter(nn.init.xavier_uniform_(torch.Tensor(1, num_inputs)))\n","    self.bias = nn.Parameter((torch.randn(1)))\n","\n","  def propagate(self, inputs):\n","    output = torch.dot(inputs, self.weights.view(-1)) + self.bias\n","    return output\n","\n","class MultiLayerPerceptron(nn.Module):\n","  def __init__(self):\n","    super(MultiLayerPerceptron, self).__init__()\n","    self.hidden_layer1 = nn.ModuleList([Perceptron(28*28) for i in range(128)])\n","    self.hidden_layer2 = nn.ModuleList([Perceptron(128) for i in range(64)])\n","    self.output_layer = nn.ModuleList([Perceptron(64) for i in range(10)])\n","\n","  def propagate(self, inputs):\n","        # Flatten the input data\n","        inputs = inputs.view(-1, 28*28)\n","\n","        # Store the matrices of weights\n","        hidden1_weights = torch.stack([perceptron.weights for perceptron in self.hidden_layer1])\n","        hidden1_weights = hidden1_weights.squeeze(dim=1)\n","        hidden2_weights = torch.stack([perceptron.weights for perceptron in self.hidden_layer2])\n","        hidden2_weights = hidden2_weights.squeeze(dim=1)\n","        output_weights = torch.stack([perceptron.weights for perceptron in self.output_layer])\n","        output_weights = output_weights.squeeze(dim=1)\n","\n","        # Store the vectors of biases\n","        hidden1_bias = torch.stack([perceptron.bias for perceptron in self.hidden_layer1])\n","        hidden1_bias = hidden1_bias.view(-1)\n","        hidden2_bias = torch.stack([perceptron.bias for perceptron in self.hidden_layer2])\n","        hidden2_bias = hidden2_bias.view(-1)\n","        output_bias = torch.stack([perceptron.bias for perceptron in self.output_layer])\n","        output_bias = output_bias.view(-1)\n","\n","        # Creating output list\n","        outputs_list = []\n","\n","        for picture, pixels in enumerate(inputs):\n","          pixels = torch.relu(F.linear(pixels, hidden1_weights, hidden1_bias))\n","          pixels = torch.relu(F.linear(pixels, hidden2_weights, hidden2_bias))\n","          outputs_list.append(F.linear(pixels, output_weights, output_bias))\n","\n","        # Concatenate the output tensors along the 0th dimension\n","        output = torch.stack(outputs_list)\n","        return output.squeeze()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f77qXmJkdm98","outputId":"786ebb6f-1c04-4ce8-e53d-1afef7a6fe10","executionInfo":{"status":"ok","timestamp":1697086297239,"user_tz":-360,"elapsed":3742925,"user":{"displayName":"Zaed Khan","userId":"13504418901449263064"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1  Loss: 0.5601648842332079\n","Epoch: 2  Loss: 0.3081548534619656\n","Epoch: 3  Loss: 0.26011377833545335\n","Epoch: 4  Loss: 0.22806015506243782\n","Epoch: 5  Loss: 0.2031794820330354\n","Epoch: 6  Loss: 0.1830284744659975\n","Epoch: 7  Loss: 0.1672863937568849\n","Epoch: 8  Loss: 0.1538584763482054\n","Epoch: 9  Loss: 0.14243584793053074\n","Epoch: 10  Loss: 0.13199058028339133\n","Epoch: 1  Loss: 0.5649746936092626\n","Epoch: 2  Loss: 0.30023697545247546\n","Epoch: 3  Loss: 0.25186321703689313\n","Epoch: 4  Loss: 0.2187752928068516\n","Epoch: 5  Loss: 0.1955351133598511\n","Epoch: 6  Loss: 0.17745279865081248\n","Epoch: 7  Loss: 0.16270116122880343\n","Epoch: 8  Loss: 0.14964200557866839\n","Epoch: 9  Loss: 0.13955957656388662\n","Epoch: 10  Loss: 0.12994932497480213\n","Epoch: 1  Loss: 0.5845752139208413\n","Epoch: 2  Loss: 0.31106119013544337\n","Epoch: 3  Loss: 0.2629405044193969\n","Epoch: 4  Loss: 0.2311842833151187\n","Epoch: 5  Loss: 0.2077137244734238\n","Epoch: 6  Loss: 0.1882971264719804\n","Epoch: 7  Loss: 0.17245602893279688\n","Epoch: 8  Loss: 0.15918436561868007\n","Epoch: 9  Loss: 0.14787809864512638\n","Epoch: 10  Loss: 0.13773155201083498\n","Epoch: 1  Loss: 0.55910821970719\n","Epoch: 2  Loss: 0.29724134051246937\n","Epoch: 3  Loss: 0.25101014271156113\n","Epoch: 4  Loss: 0.22051756723777954\n","Epoch: 5  Loss: 0.19840073596630523\n","Epoch: 6  Loss: 0.18086632896564217\n","Epoch: 7  Loss: 0.16558648760257755\n","Epoch: 8  Loss: 0.15295653656792285\n","Epoch: 9  Loss: 0.14312018625644732\n","Epoch: 10  Loss: 0.13257019815128496\n","Epoch: 1  Loss: 0.5826773770264725\n","Epoch: 2  Loss: 0.3130020467258656\n","Epoch: 3  Loss: 0.26278584431420005\n","Epoch: 4  Loss: 0.22953940460136704\n","Epoch: 5  Loss: 0.2038030477363799\n","Epoch: 6  Loss: 0.184619594468618\n","Epoch: 7  Loss: 0.1684172020804113\n","Epoch: 8  Loss: 0.15533641855647443\n","Epoch: 9  Loss: 0.14362419729452652\n","Epoch: 10  Loss: 0.1344945331209345\n","Epoch: 1  Loss: 0.5430467499535221\n","Epoch: 2  Loss: 0.2997514647461458\n","Epoch: 3  Loss: 0.2512195776067754\n","Epoch: 4  Loss: 0.2198156568406424\n","Epoch: 5  Loss: 0.19618253405890992\n","Epoch: 6  Loss: 0.17742535716760705\n","Epoch: 7  Loss: 0.16188389532117128\n","Epoch: 8  Loss: 0.14940244395500307\n","Epoch: 9  Loss: 0.13736220072907235\n","Epoch: 10  Loss: 0.12844736463944317\n","Epoch: 1  Loss: 0.6066629080169363\n","Epoch: 2  Loss: 0.31803044668043345\n","Epoch: 3  Loss: 0.26830153537393886\n","Epoch: 4  Loss: 0.23559117359695023\n","Epoch: 5  Loss: 0.2091386655548107\n","Epoch: 6  Loss: 0.18823336174429606\n","Epoch: 7  Loss: 0.17162120651835\n","Epoch: 8  Loss: 0.15727359637705438\n","Epoch: 9  Loss: 0.145646633655786\n","Epoch: 10  Loss: 0.13569181021660376\n","Epoch: 1  Loss: 0.5716672588004741\n","Epoch: 2  Loss: 0.3001102105513819\n","Epoch: 3  Loss: 0.25170309821775216\n","Epoch: 4  Loss: 0.2208817813601067\n","Epoch: 5  Loss: 0.1977289688707923\n","Epoch: 6  Loss: 0.17985076884399537\n","Epoch: 7  Loss: 0.16514563440942942\n","Epoch: 8  Loss: 0.1522124615718307\n","Epoch: 9  Loss: 0.1416561332668291\n","Epoch: 10  Loss: 0.132685103368308\n","Epoch: 1  Loss: 0.5551530579323454\n","Epoch: 2  Loss: 0.3069100868838555\n","Epoch: 3  Loss: 0.2580081000368097\n","Epoch: 4  Loss: 0.2255950589408117\n","Epoch: 5  Loss: 0.20052983419997478\n","Epoch: 6  Loss: 0.17990071856692783\n","Epoch: 7  Loss: 0.16335848238724254\n","Epoch: 8  Loss: 0.1504992170414246\n","Epoch: 9  Loss: 0.1389267182728248\n","Epoch: 10  Loss: 0.12926833267047652\n","Epoch: 1  Loss: 0.5366747030761959\n","Epoch: 2  Loss: 0.29727870670717155\n","Epoch: 3  Loss: 0.24642372529691597\n","Epoch: 4  Loss: 0.21320122850935724\n","Epoch: 5  Loss: 0.18918326372769215\n","Epoch: 6  Loss: 0.17062263359337537\n","Epoch: 7  Loss: 0.15589817209856344\n","Epoch: 8  Loss: 0.14437473504377135\n","Epoch: 9  Loss: 0.1344050073297198\n","Epoch: 10  Loss: 0.12563209276177736\n"]}],"source":["weight_distribution = []\n","bias_distribution = []\n","number_experiments = 10\n","\n","for experiment in range(number_experiments):\n","  # Model Training\n","  model = MultiLayerPerceptron()\n","  loss_function = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(model.parameters(), lr = 0.01)\n","  num_epochs = 10\n","  num_neurons = 202\n","  num_parameters = num_neurons*2 # For weights and biases\n","\n","  weight_history = [[[] for _ in range(num_parameters)] for _ in range(num_epochs)]\n","  bias_history = [[[] for _ in range(num_parameters)] for _ in range(num_epochs)]\n","\n","  for epoch in range(num_epochs):\n","      epoch_loss = 0\n","      for batch in trainloader:\n","          images, labels = batch\n","          optimizer.zero_grad()\n","          outputs = model.propagate(images)\n","          loss = loss_function(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","          epoch_loss += loss.item()\n","      for idx, param in enumerate(model.named_parameters()):\n","          if idx%2!=1:\n","              weight_history[epoch][idx].append(param[1].data.clone())\n","          else:\n","              bias_history[epoch][idx].append(param[1].data.clone())\n","\n","      print(\"Epoch:\", epoch + 1, \" Loss:\", epoch_loss / len(trainloader))\n","\n","  weight_distribution.append(weight_history)\n","  bias_distribution.append(bias_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7aKs1Xdrok2c","executionInfo":{"status":"ok","timestamp":1697086300152,"user_tz":-360,"elapsed":2968,"user":{"displayName":"Zaed Khan","userId":"13504418901449263064"}},"outputId":"023daa6c-fea6-4b9d-bb90-712bd3f300bc"},"outputs":[{"output_type":"stream","name":"stdout","text":["95.61 %\n"]}],"source":["# Model Testing\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for batch in testloader:\n","    images, labels = batch\n","    outputs = model.propagate(images)\n","    _,predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","accuracy1 = (correct / total)*100\n","print(accuracy1, \"%\")"]},{"cell_type":"markdown","metadata":{"id":"-c8wdeyjZe2n"},"source":["#Kernel Distribution Estimation"]},{"cell_type":"markdown","metadata":{"id":"I5LB4Lcrtwz_"},"source":["## Gaussian Kernel"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5cxDHAHdZiGV"},"outputs":[],"source":["# # Importing the necessary libraries\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","# from scipy.stats import gaussian_kde\n","\n","# # Bandwidth parameter (adjust this)\n","# bandwidth = 0.1\n","\n","# def plot_kde(weights, name):\n","#     plt.figure(figsize=(8, 4))\n","#     plt.title(f'{name} - Epoch 1 - 10')\n","\n","#     for epoch in range(num_epochs):\n","#         # Create a KDE object with Gaussian kernel\n","#         kde = gaussian_kde(weights[epoch][0][0], bw_method=bandwidth)\n","\n","#         # Define a range of x values for the PDF\n","#         x_values = np.linspace(min(weights[epoch][0][0]), max(weights[epoch][0][0]), 1000)\n","\n","#         # Calculate the estimated PDF at each x value\n","#         pdf_values = kde(x_values)\n","\n","#         # Plot the KDE estimate\n","#         plt.plot(x_values, pdf_values, label=f'Epoch {epoch + 1}')\n","#     # plt.title(f'KDE with Gaussian Kernel - Epoch {epoch + 1}')\n","#     plt.xlabel('Weight')\n","#     plt.ylabel('Estimated Density')\n","#     plt.legend()\n","#     plt.show()\n","\n","# # Loop through weight distributions for each epoch\n","# for idx, (name, param) in enumerate(model.named_parameters()):\n","#   if 'weight' in name:\n","#       plot_kde([weight_history[i][idx] for i in range(num_epochs)], name)"]},{"cell_type":"markdown","metadata":{"id":"Ujp8LJLbro3n"},"source":["# KL Divergence of Weight Distributions among Neurons per Layer per Epoch"]},{"cell_type":"markdown","metadata":{"id":"HE27cea5AdLK"},"source":["## Standard Deviation"]},{"cell_type":"markdown","metadata":{"id":"2Th_fc3weUOz"},"source":["### Top Down"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"83KlzKFOD2xA"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# import torch\n","# from scipy.special import kl_div\n","\n","# # Initialize lists to store standard deviations for each layer across epochs\n","# std_deviations_hidden1 = []\n","# std_deviations_hidden2 = []\n","# std_deviations_output = []\n","\n","# # Loop over each epoch - 10 times\n","# for epoch in range(num_epochs):\n","#     kl_divergences = [[0.0] * 127, [0.0] * 63, [0.0] * 9]  # Store KL divergences for each layer\n","#     for experiment in range(number_experiments):\n","#       hidden1_idx = 0\n","#       hidden2_idx = 0\n","#       output_idx = 0\n","#       for idx, (name, param) in enumerate(model.named_parameters()):\n","#           if 'weight' in name:\n","#               if 'hidden_layer1' in name:\n","#                   if idx < 254:\n","#                       # Calculate the KL divergence\n","#                       kl_divergence = kl_div(weight_distribution[experiment][epoch][idx][0][0],\n","#                                             weight_distribution[experiment][epoch][idx + 2][0][0])\n","#                       kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#                       kl_divergence = sum(kl_divergence)\n","#                       kl_divergences[0][hidden1_idx] += kl_divergence\n","#                       hidden1_idx += 1\n","#               elif 'hidden_layer2' in name:\n","#                   if idx < 382:\n","#                       # Calculate the KL divergence\n","#                       kl_divergence = kl_div(weight_distribution[experiment][epoch][idx][0][0],\n","#                                             weight_distribution[experiment][epoch][idx + 2][0][0])\n","#                       kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#                       kl_divergence = sum(kl_divergence)\n","#                       kl_divergences[1][hidden2_idx] += kl_divergence\n","#                       hidden2_idx += 1\n","#               elif 'output_layer' in name:\n","#                   if idx < 402:\n","#                       # Calculate the KL divergence\n","#                       kl_divergence = kl_div(weight_distribution[experiment][epoch][idx][0][0],\n","#                                             weight_distribution[experiment][epoch][idx + 2][0][0])\n","#                       kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#                       kl_divergence = sum(kl_divergence)\n","#                       kl_divergences[2][output_idx] += kl_divergence\n","#                       output_idx += 1\n","\n","#     # Divide each KL divergence by number_experiments (number of experiments)\n","#     for i in range(3):\n","#         kl_divergences[i] = [kl / number_experiments for kl in kl_divergences[i]]\n","\n","#     # Calculate the standard deviation for each layer and store it for this epoch\n","#     std_hidden1 = torch.std(torch.tensor(kl_divergences[0]))\n","#     std_hidden2 = torch.std(torch.tensor(kl_divergences[1]))\n","#     std_output = torch.std(torch.tensor(kl_divergences[2]))\n","\n","#     std_deviations_hidden1.append(std_hidden1.item())\n","#     std_deviations_hidden2.append(std_hidden2.item())\n","#     std_deviations_output.append(std_output.item())\n","\n","# # Create a single subplot for all three layers\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","\n","# # Plot standard deviation of KL divergence for Hidden Layer 1\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden1, marker='o', linestyle='-', color='blue', label='Hidden Layer 1')\n","\n","# # Plot standard deviation of KL divergence for Hidden Layer 2\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden2, marker='o', linestyle='-', color='green', label='Hidden Layer 2')\n","\n","# # Plot standard deviation of KL divergence for Output Layer\n","# # ax.plot(range(1, num_epochs + 1), std_deviations_output, marker='o', linestyle='-', color='red', label='Output Layer')\n","\n","# # Set titles and labels\n","# ax.set_title('Standard Deviation of KL Divergence - Top Down')\n","# ax.set_xlabel('Epoch')\n","# ax.set_ylabel('Standard Deviation')\n","\n","# # Add a legend\n","# ax.legend()\n","\n","# # Show the plot\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sGZdYOFmeZ8g"},"source":["### Bottom Up"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EXErvvtFeYyP"},"outputs":[],"source":["# # Initialize lists to store standard deviations for each layer across epochs\n","# std_deviations_hidden1_reverse = []\n","# std_deviations_hidden2_reverse = []\n","# std_deviations_output_reverse = []\n","\n","# # Loop over each epoch - 10 times\n","# for epoch in range(num_epochs):\n","#     kl_divergences_reverse = [[0.0] * 127, [0.0] * 63, [0.0] * 9]  # Store KL divergences for each layer\n","#     for experiment in range(number_experiments):\n","#       hidden1_idx = 0\n","#       hidden2_idx = 0\n","#       output_idx = 0\n","#       for idx, (name, param) in reversed(list(enumerate(model.named_parameters()))):\n","#           if 'weight' in name:\n","#               if 'hidden_layer1' in name:\n","#                   if idx > 3:\n","#                       # Calculate the KL divergence\n","#                       kl_divergence_reverse = kl_div(weight_distribution[experiment][epoch][idx][0][0],\n","#                                             weight_distribution[experiment][epoch][idx - 2][0][0])\n","#                       kl_divergence_reverse = [kl.item() for kl in kl_divergence_reverse if kl != float('inf')]\n","#                       kl_divergence_reverse = sum(kl_divergence_reverse)\n","#                       kl_divergences_reverse[0][hidden1_idx] += kl_divergence_reverse\n","#                       hidden1_idx += 1\n","#               elif 'hidden_layer2' in name:\n","#                   if idx > 256:\n","#                       # Calculate the KL divergence\n","#                       kl_divergence_reverse = kl_div(weight_distribution[experiment][epoch][idx][0][0],\n","#                                             weight_distribution[experiment][epoch][idx - 2][0][0])\n","#                       kl_divergence_reverse = [kl.item() for kl in kl_divergence_reverse if kl != float('inf')]\n","#                       kl_divergence_reverse = sum(kl_divergence_reverse)\n","#                       kl_divergences_reverse[1][hidden2_idx] += kl_divergence_reverse\n","#                       hidden2_idx += 1\n","#               elif 'output_layer' in name:\n","#                   if idx > 384:\n","#                       # Calculate the KL divergence\n","#                       kl_divergence_reverse = kl_div(weight_distribution[experiment][epoch][idx][0][0],\n","#                                             weight_distribution[experiment][epoch][idx - 2][0][0])\n","#                       kl_divergence_reverse = [kl.item() for kl in kl_divergence_reverse if kl != float('inf')]\n","#                       kl_divergence_reverse = sum(kl_divergence_reverse)\n","#                       kl_divergences_reverse[2][output_idx] += kl_divergence_reverse\n","#                       output_idx += 1\n","\n","#     # Divide each KL divergence by number_experiments (number of experiments)\n","#     for i in range(3):\n","#         kl_divergences_reverse[i] = [kl / number_experiments for kl in kl_divergences_reverse[i]]\n","\n","#     # Calculate the standard deviation for each layer and store it for this epoch\n","#     std_hidden1_reverse = torch.std(torch.tensor(kl_divergences_reverse[0]))\n","#     std_hidden2_reverse = torch.std(torch.tensor(kl_divergences_reverse[1]))\n","#     std_output_reverse = torch.std(torch.tensor(kl_divergences_reverse[2]))\n","\n","#     std_deviations_hidden1_reverse.append(std_hidden1_reverse.item())\n","#     std_deviations_hidden2_reverse.append(std_hidden2_reverse.item())\n","#     std_deviations_output_reverse.append(std_output_reverse.item())\n","\n","# # Create a single subplot for all three layers\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","\n","# # Plot standard deviation of KL divergence for Hidden Layer 1\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden1_reverse, marker='o', linestyle='-', color='blue', label='Hidden Layer 1')\n","\n","# # Plot standard deviation of KL divergence for Hidden Layer 2\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden2_reverse, marker='o', linestyle='-', color='green', label='Hidden Layer 2')\n","\n","# # Plot standard deviation of KL divergence for Output Layer\n","# # ax.plot(range(1, num_epochs + 1), std_deviations_output_reverse, marker='o', linestyle='-', color='red', label='Output Layer')\n","\n","# # Set titles and labels\n","# ax.set_title('Standard Deviation of KL Divergence for All Layers')\n","# ax.set_xlabel('Epoch')\n","# ax.set_ylabel('Standard Deviation')\n","\n","# # Add a legend\n","# ax.legend()\n","\n","# # Show the plot\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"saDZN3GiindC"},"outputs":[],"source":["# # Hidden Layer 1\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden1, marker='o', linestyle='-', color='blue', label='Top Down')\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden1_reverse, marker='o', linestyle='-', color='green', label='Bottom Up')\n","\n","# # Set titles and labels\n","# ax.set_title('Standard Deviation of KL Divergence for Hidden Layer 1')\n","# ax.set_xlabel('Epoch')\n","# ax.set_ylabel('Standard Deviation')\n","\n","# # Add a legend\n","# ax.legend()\n","\n","# # Show the plot\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wkrmDXtjY2x"},"outputs":[],"source":["# # Hidden Layer 2\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden2, marker='o', linestyle='-', color='blue', label='Top Down')\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden2_reverse, marker='o', linestyle='-', color='green', label='Bottom Up')\n","\n","# # Set titles and labels\n","# ax.set_title('Standard Deviation of KL Divergence for Hidden Layer 2')\n","# ax.set_xlabel('Epoch')\n","# ax.set_ylabel('Standard Deviation')\n","\n","# # Add a legend\n","# ax.legend()\n","\n","# # Show the plot\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"08-KFA1AtXN2"},"outputs":[],"source":["# from scipy.special import kl_div\n","\n","# # Loop over each epoch - 10 times\n","# for epoch in range(num_epochs):\n","#   kl_divergences = [[0.0]*127, [0.0]*63, [0.0]*9]\n","#   hidden1_idx = 0\n","#   hidden2_idx = 0\n","#   output_idx = 0\n","#   for idx, (name, param) in enumerate(model.named_parameters()):\n","#     if 'weight' in name:\n","#       if 'hidden_layer1' in name:\n","#         if idx < 254:\n","#           # Calculate the KL divergence\n","#           kl_divergence = kl_div(weight_history[epoch][idx][0][0], weight_history[epoch][idx+2][0][0])\n","#           kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#           kl_divergence = sum(kl_divergence)\n","#           kl_divergences[0][hidden1_idx] = kl_divergence\n","#           hidden1_idx+=1\n","#       elif 'hidden_layer2' in name:\n","#         if idx < 382:\n","#           # Calculate the KL divergence\n","#           kl_divergence = kl_div(weight_history[epoch][idx][0][0], weight_history[epoch][idx+2][0][0])\n","#           kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#           kl_divergence = sum(kl_divergence)\n","#           kl_divergences[1][hidden2_idx] = kl_divergence\n","#           hidden2_idx+=1\n","#       elif 'output_layer' in name:\n","#         if idx < 402:\n","#           # Calculate the KL divergence\n","#           kl_divergence = kl_div(weight_history[epoch][idx][0][0], weight_history[epoch][idx+2][0][0])\n","#           kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#           kl_divergence = sum(kl_divergence)\n","#           kl_divergences[2][output_idx] = kl_divergence\n","#           output_idx+=1\n","\n","#       # Define the neuron numbers\n","#   neuron_numbers_hidden1 = range(1, 128)\n","#   neuron_numbers_hidden2 = range(1, 64)\n","#   neuron_numbers_output = range(1, 10)\n","\n","#   # Create subplots for each layer\n","#   fig, axs = plt.subplots(3, figsize=(8, 12))\n","\n","#   # Plot KL divergences for hidden1 layer\n","#   axs[0].plot(neuron_numbers_hidden1, kl_divergences[0], marker='o', linestyle='-', color='blue')\n","#   axs[0].set_title('KL Divergence for Hidden Layer 1')\n","#   axs[0].set_xlabel('Neuron Number')\n","#   axs[0].set_ylabel('KL Divergence')\n","\n","#   # Plot KL divergences for hidden2 layer\n","#   axs[1].plot(neuron_numbers_hidden2, kl_divergences[1], marker='o', linestyle='-', color='green')\n","#   axs[1].set_title('KL Divergence for Hidden Layer 2')\n","#   axs[1].set_xlabel('Neuron Number')\n","#   axs[1].set_ylabel('KL Divergence')\n","\n","#   # Plot KL divergences for output layer\n","#   axs[2].plot(neuron_numbers_output, kl_divergences[2], marker='o', linestyle='-', color='red')\n","#   axs[2].set_title('KL Divergence for Output Layer')\n","#   axs[2].set_xlabel('Neuron Number')\n","#   axs[2].set_ylabel('KL Divergence')\n","\n","#   # Add a title above all subplots with the epoch number\n","#   plt.suptitle(f'Epoch {epoch + 1}')\n","\n","#   # Adjust layout\n","#   plt.tight_layout()\n","\n","#   # Show the plot\n","#   plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Fa0TDk8S6d3x"},"source":["# Prebuilt MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7IUCMoQP6fpD"},"outputs":[],"source":["# # Define a simple feedforward neural network\n","# class NeuralNetwork(nn.Module):\n","#     def __init__(self):\n","#         super(NeuralNetwork, self).__init__()\n","#         self.fc1 = nn.Linear(28 * 28, 128)  # Input: 28x28 image, Output: 128\n","#         self.fc2 = nn.Linear(128, 64)      # Hidden layer: 128 -> 64\n","#         self.fc3 = nn.Linear(64, 10)      # Output: 32 -> 10 (10 classes for MNIST)\n","\n","#     def forward(self, x):\n","#         x = x.view(-1, 28 * 28)  # Flatten the input\n","#         x = torch.relu(self.fc1(x))\n","#         x = torch.relu(self.fc2(x))\n","#         x = self.fc3(x)\n","#         return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nmsBqfEy8Eid"},"outputs":[],"source":["# weight_distribution2 = []\n","# bias_distribution2 = []\n","# number_experiments = 1\n","\n","# for experiment in range(number_experiments):\n","#   # Model Training\n","#   model2 = NeuralNetwork()\n","#   loss_function = nn.CrossEntropyLoss()\n","#   optimizer = optim.SGD(model2.parameters(), lr = 0.01)\n","#   num_epochs = 10\n","#   num_neurons = 202\n","#   num_parameters = num_neurons*2 # For weights and biases\n","\n","#   weight_history2 = [[[] for _ in range(num_parameters)] for _ in range(num_epochs)]\n","#   bias_history2 = [[[] for _ in range(num_parameters)] for _ in range(num_epochs)]\n","\n","#   for epoch in range(num_epochs):\n","#       epoch_loss = 0\n","#       for batch in trainloader:\n","#           images, labels = batch\n","#           optimizer.zero_grad()\n","#           outputs = model2(images)\n","#           loss = loss_function(outputs, labels)\n","#           loss.backward()\n","#           optimizer.step()\n","#           epoch_loss += loss.item()\n","#       for idx, param in enumerate(model2.named_parameters()):\n","#           if idx%2!=1:\n","#               weight_history2[epoch][idx].append(param[1].data.clone())\n","#           else:\n","#               bias_history2[epoch][idx].append(param[1].data.clone())\n","\n","#       print(\"Epoch:\", epoch + 1, \" Loss:\", epoch_loss / len(trainloader))\n","\n","#   weight_distribution2.append(weight_history2)\n","#   bias_distribution2.append(bias_history2)"]},{"cell_type":"code","source":["# # Model Testing\n","# correct = 0\n","# total = 0\n","# with torch.no_grad():\n","#   for batch in testloader:\n","#     images, labels = batch\n","#     outputs = model2(images)\n","#     _,predicted = torch.max(outputs.data, 1)\n","#     total += labels.size(0)\n","#     correct += (predicted == labels).sum().item()\n","# accuracy2 = (correct / total)*100\n","# print(accuracy2, \"%\")"],"metadata":{"id":"33T3j1bcweOt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gVPghfAb_nA3"},"outputs":[],"source":["# import matplotlib.pyplot as plt\n","# import torch\n","# from scipy.special import kl_div\n","\n","# # Initialize lists to store standard deviations for each layer across epochs\n","# std_deviations_hidden1 = []\n","# std_deviations_hidden2 = []\n","# std_deviations_output = []\n","\n","# # Loop over each epoch - 10 times\n","# for epoch in range(num_epochs):\n","#     kl_divergences = [[0.0] * 127, [0.0] * 63, [0.0] * 9]  # Store KL divergences for each layer\n","#     for experiment in range(number_experiments):\n","#       hidden1_idx = 0\n","#       hidden2_idx = 0\n","#       output_idx = 0\n","#       for layer in range(0,5,2):\n","#         if layer == 0:\n","#           for neuron in range(127):\n","#             # Calculate the KL divergence\n","#             kl_divergence = kl_div(weight_distribution[experiment][epoch][layer][0][neuron],\n","#                                   weight_distribution[experiment][epoch][layer][0][neuron+1])\n","#             kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#             kl_divergence = sum(kl_divergence)\n","#             kl_divergences[0][hidden1_idx] += kl_divergence\n","#             hidden1_idx += 1\n","#         if layer == 2:\n","#           for neuron in range(63):\n","#             # Calculate the KL divergence\n","#             kl_divergence = kl_div(weight_distribution[experiment][epoch][layer][0][neuron],\n","#                                   weight_distribution[experiment][epoch][layer][0][neuron+1])\n","#             kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#             kl_divergence = sum(kl_divergence)\n","#             kl_divergences[1][hidden2_idx] += kl_divergence\n","#             hidden2_idx += 1\n","#         if layer == 4:\n","#           for neuron in range(9):\n","#             # Calculate the KL divergence\n","#             kl_divergence = kl_div(weight_distribution[experiment][epoch][layer][0][neuron],\n","#                                   weight_distribution[experiment][epoch][layer][0][neuron+1])\n","#             kl_divergence = [kl.item() for kl in kl_divergence if kl != float('inf')]\n","#             kl_divergence = sum(kl_divergence)\n","#             kl_divergences[2][output_idx] += kl_divergence\n","#             output_idx += 1\n","\n","#     # Divide each KL divergence by number_experiments (number of experiments)\n","#     for i in range(3):\n","#         kl_divergences[i] = [kl / number_experiments for kl in kl_divergences[i]]\n","\n","#     # Calculate the standard deviation for each layer and store it for this epoch\n","#     std_hidden1 = torch.std(torch.tensor(kl_divergences[0]))\n","#     std_hidden2 = torch.std(torch.tensor(kl_divergences[1]))\n","#     std_output = torch.std(torch.tensor(kl_divergences[2]))\n","\n","#     std_deviations_hidden1.append(std_hidden1.item())\n","#     std_deviations_hidden2.append(std_hidden2.item())\n","#     std_deviations_output.append(std_output.item())\n","\n","# # Create a single subplot for all three layers\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","\n","# # Plot standard deviation of KL divergence for Hidden Layer 1\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden1, marker='o', linestyle='-', color='blue', label='Hidden Layer 1')\n","\n","# # Plot standard deviation of KL divergence for Hidden Layer 2\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden2, marker='o', linestyle='-', color='green', label='Hidden Layer 2')\n","\n","# # Plot standard deviation of KL divergence for Output Layer\n","# # ax.plot(range(1, num_epochs + 1), std_deviations_output, marker='o', linestyle='-', color='red', label='Output Layer')\n","\n","# # Set titles and labels\n","# ax.set_title('Standard Deviation of KL Divergence - Top Down')\n","# ax.set_xlabel('Epoch')\n","# ax.set_ylabel('Standard Deviation')\n","\n","# # Add a legend\n","# ax.legend()\n","\n","# # Show the plot\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbNLvdprTthL"},"outputs":[],"source":["# # Initialize lists to store standard deviations for each layer across epochs\n","# std_deviations_hidden1_reverse = []\n","# std_deviations_hidden2_reverse = []\n","# std_deviations_output_reverse = []\n","\n","# # Loop over each epoch - 10 times\n","# for epoch in range(num_epochs):\n","#     kl_divergences_reverse = [[0.0] * 127, [0.0] * 63, [0.0] * 9]  # Store KL divergences for each layer\n","#     for experiment in range(number_experiments):\n","#       hidden1_idx = 0\n","#       hidden2_idx = 0\n","#       output_idx = 0\n","#       for layer in range(0,5,2):\n","#         if layer == 0:\n","#           for neuron in range(127, 1, -1):\n","#             # Calculate the KL divergence\n","#             kl_divergence_reverse = kl_div(weight_distribution[experiment][epoch][layer][0][neuron],\n","#                                   weight_distribution[experiment][epoch][layer][0][neuron-1])\n","#             kl_divergence_reverse = [kl.item() for kl in kl_divergence_reverse if kl != float('inf')]\n","#             kl_divergence_reverse = sum(kl_divergence_reverse)\n","#             kl_divergences_reverse[0][hidden1_idx] += kl_divergence_reverse\n","#             hidden1_idx += 1\n","#         if layer == 2:\n","#           for neuron in range(63, 1, -1):\n","#             # Calculate the KL divergence\n","#             kl_divergence_reverse = kl_div(weight_distribution[experiment][epoch][layer][0][neuron],\n","#                                   weight_distribution[experiment][epoch][layer][0][neuron-1])\n","#             kl_divergence_reverse = [kl.item() for kl in kl_divergence_reverse if kl != float('inf')]\n","#             kl_divergence_reverse = sum(kl_divergence_reverse)\n","#             kl_divergences_reverse[1][hidden2_idx] += kl_divergence_reverse\n","#             hidden2_idx += 1\n","#         if layer == 4:\n","#           for neuron in range(9, 1, -1):\n","#             # Calculate the KL divergence\n","#             kl_divergence_reverse = kl_div(weight_distribution[experiment][epoch][layer][0][neuron],\n","#                                   weight_distribution[experiment][epoch][layer][0][neuron-1])\n","#             kl_divergence_reverse = [kl.item() for kl in kl_divergence_reverse if kl != float('inf')]\n","#             kl_divergence_reverse = sum(kl_divergence_reverse)\n","#             kl_divergences_reverse[2][output_idx] += kl_divergence_reverse\n","#             output_idx += 1\n","\n","#     # Divide each KL divergence by number_experiments (number of experiments)\n","#     for i in range(3):\n","#         kl_divergences_reverse[i] = [kl / number_experiments for kl in kl_divergences_reverse[i]]\n","\n","#     # Calculate the standard deviation for each layer and store it for this epoch\n","#     std_hidden1_reverse = torch.std(torch.tensor(kl_divergences_reverse[0]))\n","#     std_hidden2_reverse = torch.std(torch.tensor(kl_divergences_reverse[1]))\n","#     std_output_reverse = torch.std(torch.tensor(kl_divergences_reverse[2]))\n","\n","#     std_deviations_hidden1_reverse.append(std_hidden1_reverse.item())\n","#     std_deviations_hidden2_reverse.append(std_hidden2_reverse.item())\n","#     std_deviations_output_reverse.append(std_output_reverse.item())\n","\n","# # Create a single subplot for all three layers\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","\n","# # Plot standard deviation of KL divergence for Hidden Layer 1\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden1_reverse, marker='o', linestyle='-', color='blue', label='Hidden Layer 1')\n","\n","# # Plot standard deviation of KL divergence for Hidden Layer 2\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden2_reverse, marker='o', linestyle='-', color='green', label='Hidden Layer 2')\n","\n","# # Plot standard deviation of KL divergence for Output Layer\n","# # ax.plot(range(1, num_epochs + 1), std_deviations_output, marker='o', linestyle='-', color='red', label='Output Layer')\n","\n","# # Set titles and labels\n","# ax.set_title('Standard Deviation of KL Divergence - Bottom Up')\n","# ax.set_xlabel('Epoch')\n","# ax.set_ylabel('Standard Deviation')\n","\n","# # Add a legend\n","# ax.legend()\n","\n","# # Show the plot\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9MNZYzgBh5k5"},"outputs":[],"source":["# # Hidden Layer 1\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden1, marker='o', linestyle='-', color='blue', label='Top Down')\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden1_reverse, marker='o', linestyle='-', color='green', label='Bottom Up')\n","\n","# # Set titles and labels\n","# ax.set_title('Standard Deviation of KL Divergence for Hidden Layer 1 - Prebuilt')\n","# ax.set_xlabel('Epoch')\n","# ax.set_ylabel('Standard Deviation')\n","\n","# # Add a legend\n","# ax.legend()\n","\n","# # Show the plot\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbhzJKpBiAcp"},"outputs":[],"source":["# # Hidden Layer 2\n","# fig, ax = plt.subplots(figsize=(8, 6))\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden2, marker='o', linestyle='-', color='blue', label='Top Down')\n","# ax.plot(range(1, num_epochs + 1), std_deviations_hidden2_reverse, marker='o', linestyle='-', color='green', label='Bottom Up')\n","\n","# # Set titles and labels\n","# ax.set_title('Standard Deviation of KL Divergence for Hidden Layer 2 - Prebuilt')\n","# ax.set_xlabel('Epoch')\n","# ax.set_ylabel('Standard Deviation')\n","\n","# # Add a legend\n","# ax.legend()\n","\n","# # Show the plot\n","# plt.tight_layout()\n","# plt.show()"]},{"cell_type":"markdown","source":["# Custom MLP - with Dropout"],"metadata":{"id":"fWsh0tU5t09C"}},{"cell_type":"code","source":["class MultiLayerPerceptron2(nn.Module):\n","  def __init__(self):\n","    super(MultiLayerPerceptron2, self).__init__()\n","    self.hidden_layer1 = nn.ModuleList([Perceptron(28*28) for i in range(128)])\n","    self.hidden_layer2 = nn.ModuleList([Perceptron(128) for i in range(64)])\n","    self.output_layer = nn.ModuleList([Perceptron(64) for i in range(10)])\n","\n","  def propagate(self, inputs):\n","        # Flatten the input data\n","        inputs = inputs.view(-1, 28*28)\n","\n","        # Store the matrices of weights\n","        hidden1_weights = torch.stack([perceptron.weights for perceptron in self.hidden_layer1])\n","        hidden1_weights = hidden1_weights.squeeze(dim=1)\n","        hidden2_weights = torch.stack([perceptron.weights for perceptron in self.hidden_layer2])\n","        hidden2_weights = hidden2_weights.squeeze(dim=1)\n","        output_weights = torch.stack([perceptron.weights for perceptron in self.output_layer])\n","        output_weights = output_weights.squeeze(dim=1)\n","\n","        # Store the vectors of biases\n","        hidden1_bias = torch.stack([perceptron.bias for perceptron in self.hidden_layer1])\n","        hidden1_bias = hidden1_bias.view(-1)\n","        hidden2_bias = torch.stack([perceptron.bias for perceptron in self.hidden_layer2])\n","        hidden2_bias = hidden2_bias.view(-1)\n","        output_bias = torch.stack([perceptron.bias for perceptron in self.output_layer])\n","        output_bias = output_bias.view(-1)\n","\n","        # Creating output list\n","        outputs_list = []\n","\n","        for picture, pixels in enumerate(inputs):\n","          pixels = torch.relu(F.linear(pixels, hidden1_weights, hidden1_bias))\n","          pixels = F.dropout(pixels, p=0.5, training=self.training)\n","          pixels = torch.relu(F.linear(pixels, hidden2_weights, hidden2_bias))\n","          pixels = F.dropout(pixels, p=0.5, training=self.training)\n","          outputs_list.append(F.linear(pixels, output_weights, output_bias))\n","\n","        # Concatenate the output tensors along the 0th dimension\n","        output = torch.stack(outputs_list)\n","        return output.squeeze()"],"metadata":{"id":"FPjhLWlQt4Tq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weight_distribution3 = []\n","bias_distribution3 = []\n","number_experiments = 10\n","\n","for experiment in range(number_experiments):\n","  # Model Training\n","  model3 = MultiLayerPerceptron2()\n","  loss_function = nn.CrossEntropyLoss()\n","  optimizer = optim.SGD(model3.parameters(), lr = 0.01)\n","  num_epochs = 10\n","  num_neurons = 202\n","  num_parameters = num_neurons*2 # For weights and biases\n","\n","  weight_history3 = [[[] for _ in range(num_parameters)] for _ in range(num_epochs)]\n","  bias_history3 = [[[] for _ in range(num_parameters)] for _ in range(num_epochs)]\n","\n","  for epoch in range(num_epochs):\n","      epoch_loss = 0\n","      for batch in trainloader:\n","          images, labels = batch\n","          optimizer.zero_grad()\n","          outputs = model3.propagate(images)\n","          loss = loss_function(outputs, labels)\n","          loss.backward()\n","          optimizer.step()\n","          epoch_loss += loss.item()\n","      for idx, param in enumerate(model3.named_parameters()):\n","          if idx%2!=1:\n","              weight_history3[epoch][idx].append(param[1].data.clone())\n","          else:\n","              bias_history3[epoch][idx].append(param[1].data.clone())\n","\n","      print(\"Epoch:\", epoch + 1, \" Loss:\", epoch_loss / len(trainloader))\n","\n","  weight_distribution3.append(weight_history3)\n","  bias_distribution3.append(bias_history3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GQltzNneuUZl","executionInfo":{"status":"ok","timestamp":1697265500477,"user_tz":-360,"elapsed":4641975,"user":{"displayName":"Zaed Khan","userId":"11956878547931761934"}},"outputId":"d5a93d44-aa34-417a-888f-d3a665ff8430"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1  Loss: 1.4618864338408146\n","Epoch: 2  Loss: 0.8681789220078413\n","Epoch: 3  Loss: 0.7172107178328643\n","Epoch: 4  Loss: 0.6288272968487445\n","Epoch: 5  Loss: 0.5787643726065215\n","Epoch: 6  Loss: 0.5332080299602643\n","Epoch: 7  Loss: 0.5049786285233142\n","Epoch: 8  Loss: 0.48493274773107664\n","Epoch: 9  Loss: 0.4624874217392031\n","Epoch: 10  Loss: 0.4479015975046768\n","Epoch: 1  Loss: 1.4679734059996696\n","Epoch: 2  Loss: 0.8532824613201593\n","Epoch: 3  Loss: 0.6912846051172407\n","Epoch: 4  Loss: 0.6212256017015941\n","Epoch: 5  Loss: 0.5599839412835615\n","Epoch: 6  Loss: 0.5256278378718189\n","Epoch: 7  Loss: 0.49529236361289075\n","Epoch: 8  Loss: 0.47336639049274326\n","Epoch: 9  Loss: 0.4530291231567544\n","Epoch: 10  Loss: 0.4355112289124206\n","Epoch: 1  Loss: 1.4947346072715482\n","Epoch: 2  Loss: 0.8727517576614169\n","Epoch: 3  Loss: 0.7073542041374422\n","Epoch: 4  Loss: 0.6228729527769312\n","Epoch: 5  Loss: 0.5658058591171115\n","Epoch: 6  Loss: 0.5243596357227897\n","Epoch: 7  Loss: 0.4993816325500576\n","Epoch: 8  Loss: 0.4771771633517005\n","Epoch: 9  Loss: 0.4557838345101393\n","Epoch: 10  Loss: 0.43377058786242756\n","Epoch: 1  Loss: 1.5118745505682696\n","Epoch: 2  Loss: 0.9109557043832502\n","Epoch: 3  Loss: 0.7440985173050528\n","Epoch: 4  Loss: 0.6555911844600238\n","Epoch: 5  Loss: 0.6030890088536338\n","Epoch: 6  Loss: 0.5569143073359278\n","Epoch: 7  Loss: 0.5261984536928663\n","Epoch: 8  Loss: 0.5010363269430488\n","Epoch: 9  Loss: 0.48429885492332453\n","Epoch: 10  Loss: 0.4597653213785147\n","Epoch: 1  Loss: 1.4808986243535716\n","Epoch: 2  Loss: 0.841929044232948\n","Epoch: 3  Loss: 0.6798537966729735\n","Epoch: 4  Loss: 0.6010156913892801\n","Epoch: 5  Loss: 0.5509456691266631\n","Epoch: 6  Loss: 0.5138839390168566\n","Epoch: 7  Loss: 0.4930809197713063\n","Epoch: 8  Loss: 0.47297422362288943\n","Epoch: 9  Loss: 0.44943790942398726\n","Epoch: 10  Loss: 0.43263657589647564\n","Epoch: 1  Loss: 1.550196955453104\n","Epoch: 2  Loss: 0.9243337129479023\n","Epoch: 3  Loss: 0.7539099909857646\n","Epoch: 4  Loss: 0.6594763907478817\n","Epoch: 5  Loss: 0.6035888732623443\n","Epoch: 6  Loss: 0.5622389847313417\n","Epoch: 7  Loss: 0.5285214364274479\n","Epoch: 8  Loss: 0.5008945680344536\n","Epoch: 9  Loss: 0.4835510391797592\n","Epoch: 10  Loss: 0.46347310022313964\n","Epoch: 1  Loss: 1.4319858814099196\n","Epoch: 2  Loss: 0.8384021049432917\n","Epoch: 3  Loss: 0.6952523123989227\n","Epoch: 4  Loss: 0.61872496292281\n","Epoch: 5  Loss: 0.566841802919216\n","Epoch: 6  Loss: 0.5355797388088475\n","Epoch: 7  Loss: 0.5071149229018419\n","Epoch: 8  Loss: 0.48520804704951326\n","Epoch: 9  Loss: 0.46647271548888325\n","Epoch: 10  Loss: 0.44522718375107884\n","Epoch: 1  Loss: 1.4808061648406454\n","Epoch: 2  Loss: 0.8757194103017799\n","Epoch: 3  Loss: 0.7298220094841427\n","Epoch: 4  Loss: 0.6530621980171977\n","Epoch: 5  Loss: 0.593352351615678\n","Epoch: 6  Loss: 0.549182327237846\n","Epoch: 7  Loss: 0.5209874510765076\n","Epoch: 8  Loss: 0.4928526466589238\n","Epoch: 9  Loss: 0.4741016820645027\n","Epoch: 10  Loss: 0.45504342459602903\n","Epoch: 1  Loss: 1.4231859125308137\n","Epoch: 2  Loss: 0.8345222229451768\n","Epoch: 3  Loss: 0.7040569643412572\n","Epoch: 4  Loss: 0.6296423960532715\n","Epoch: 5  Loss: 0.5776919608177152\n","Epoch: 6  Loss: 0.54311675292406\n","Epoch: 7  Loss: 0.5141316773667772\n","Epoch: 8  Loss: 0.48918985832792355\n","Epoch: 9  Loss: 0.4669511867548103\n","Epoch: 10  Loss: 0.4519513509293863\n","Epoch: 1  Loss: 1.4515243590767704\n","Epoch: 2  Loss: 0.8646216799518955\n","Epoch: 3  Loss: 0.7059748962299147\n","Epoch: 4  Loss: 0.6234071405330447\n","Epoch: 5  Loss: 0.5708001532089482\n","Epoch: 6  Loss: 0.5265007060187966\n","Epoch: 7  Loss: 0.49824027053074543\n","Epoch: 8  Loss: 0.4735020157307196\n","Epoch: 9  Loss: 0.44928173082215445\n","Epoch: 10  Loss: 0.43440625684729006\n"]}]},{"cell_type":"code","source":["# Model Testing\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","  for batch in testloader:\n","    images, labels = batch\n","    outputs = model3.propagate(images)\n","    _,predicted = torch.max(outputs.data, 1)\n","    total += labels.size(0)\n","    correct += (predicted == labels).sum().item()\n","accuracy3 = (correct / total)*100\n","print(accuracy3, \"%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sXk49LE0vUGZ","executionInfo":{"status":"ok","timestamp":1697265504232,"user_tz":-360,"elapsed":3768,"user":{"displayName":"Zaed Khan","userId":"11956878547931761934"}},"outputId":"9c966a45-4adf-4303-9e8d-30ba6736f0df"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["87.72999999999999 %\n"]}]},{"cell_type":"markdown","source":["# Prebuilt MLP - with Dropout"],"metadata":{"id":"Ug-mWuRyvbr9"}},{"cell_type":"code","source":["# # Define a simple feedforward neural network\n","# class NeuralNetwork2(nn.Module):\n","#     def __init__(self):\n","#         super(NeuralNetwork, self).__init__()\n","#         self.fc1 = nn.Linear(28 * 28, 128)  # Input: 28x28 image, Output: 128\n","#         self.fc2 = nn.Linear(128, 64)      # Hidden layer: 128 -> 64\n","#         self.fc3 = nn.Linear(64, 10)      # Output: 32 -> 10 (10 classes for MNIST)\n","\n","#     def forward(self, x):\n","#         x = x.view(-1, 28 * 28)  # Flatten the input\n","#         x = torch.relu(self.fc1(x))\n","#         x = F.dropout(x, p=0.5, training=self.training)\n","#         x = torch.relu(self.fc2(x))\n","#         x = F.dropout(x, p=0.5, training=self.training)\n","#         x = self.fc3(x)\n","#         return x"],"metadata":{"id":"B1qcDBbuvet1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# weight_distribution4 = []\n","# bias_distribution4 = []\n","# number_experiments = 1\n","\n","# for experiment in range(number_experiments):\n","#   # Model Training\n","#   model4 = NeuralNetwork()\n","#   loss_function = nn.CrossEntropyLoss()\n","#   optimizer = optim.SGD(model4.parameters(), lr = 0.01)\n","#   num_epochs = 10\n","#   num_neurons = 202\n","#   num_parameters = num_neurons*2 # For weights and biases\n","\n","#   weight_history4 = [[[] for _ in range(num_parameters)] for _ in range(num_epochs)]\n","#   bias_history4 = [[[] for _ in range(num_parameters)] for _ in range(num_epochs)]\n","\n","#   for epoch in range(num_epochs):\n","#       epoch_loss = 0\n","#       for batch in trainloader:\n","#           images, labels = batch\n","#           optimizer.zero_grad()\n","#           outputs = model4(images)\n","#           loss = loss_function(outputs, labels)\n","#           loss.backward()\n","#           optimizer.step()\n","#           epoch_loss += loss.item()\n","#       for idx, param in enumerate(model4.named_parameters()):\n","#           if idx%2!=1:\n","#               weight_history4[epoch][idx].append(param[1].data.clone())\n","#           else:\n","#               bias_history4[epoch][idx].append(param[1].data.clone())\n","\n","#       print(\"Epoch:\", epoch + 1, \" Loss:\", epoch_loss / len(trainloader))\n","\n","#   weight_distribution4.append(weight_history4)\n","#   bias_distribution4.append(bias_history4)"],"metadata":{"id":"yWSV6PrYvkgT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Model Testing\n","# correct = 0\n","# total = 0\n","# with torch.no_grad():\n","#   for batch in testloader:\n","#     images, labels = batch\n","#     outputs = model4(images)\n","#     _,predicted = torch.max(outputs.data, 1)\n","#     total += labels.size(0)\n","#     correct += (predicted == labels).sum().item()\n","# accuracy4 = (correct / total)*100\n","# print(accuracy4, \"%\")"],"metadata":{"id":"GQio6lFCvoz2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Importing the necessary libraries\n","# import numpy as np\n","# import matplotlib.pyplot as plt\n","# from scipy.stats import gaussian_kde\n","\n","# # Bandwidth parameter (adjust this)\n","# bandwidth = 0.1\n","\n","# def plot_kde(weights, name):\n","#     plt.figure(figsize=(8, 4))\n","#     plt.title(f'{name} - Epoch 1 - 10')\n","\n","#     for epoch in range(num_epochs):\n","#         # Create a KDE object with Gaussian kernel\n","#         kde = gaussian_kde(weights[epoch][0][0], bw_method=bandwidth)\n","\n","#         # Define a range of x values for the PDF\n","#         x_values = np.linspace(min(weights[epoch][0][0]), max(weights[epoch][0][0]), 1000)\n","\n","#         # Calculate the estimated PDF at each x value\n","#         pdf_values = kde(x_values)\n","\n","#         # Plot the KDE estimate\n","#         plt.plot(x_values, pdf_values, label=f'Epoch {epoch + 1}')\n","#     # plt.title(f'KDE with Gaussian Kernel - Epoch {epoch + 1}')\n","#     plt.xlabel('Weight')\n","#     plt.ylabel('Estimated Density')\n","#     plt.legend()\n","#     plt.show()"],"metadata":{"id":"xeI_outVzmQB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Loop through weight distributions for each epoch\n","# for idx, (name, param) in enumerate(model.named_parameters()):\n","#   if 'weight' in name:\n","#       plot_kde([weight_history[i][idx] for i in range(num_epochs)], name)"],"metadata":{"id":"N1X07hS7zx_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Loop through weight distributions for each epoch\n","# for idx, (name, param) in enumerate(model.named_parameters()):\n","#   if 'weight' in name:\n","#       plot_kde([weight_history2[i][idx] for i in range(num_epochs)], name)"],"metadata":{"id":"3VE48FtTzy2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Loop through weight distributions for each epoch\n","# for idx, (name, param) in enumerate(model.named_parameters()):\n","#   if 'weight' in name:\n","#       plot_kde([weight_history3[i][idx] for i in range(num_epochs)], name)"],"metadata":{"id":"jvcU-NmCzzn4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Loop through weight distributions for each epoch\n","# for idx, (name, param) in enumerate(model.named_parameters()):\n","#   if 'weight' in name:\n","#       plot_kde([weight_history4[i][idx] for i in range(num_epochs)], name)"],"metadata":{"id":"vXoPDCLMz0KJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def plot_kde2(weights1, weights2, epoch):\n","#     for idx, (name, param) in enumerate(model.named_parameters()):\n","#       if 'weight' in name:\n","#           plt.figure(figsize=(8, 4))\n","#           plt.title(f'Epoch {epoch} - {name}')\n","#           print(len(weights1))\n","#           kde1 = gaussian_kde(weights1[idx][0][0], bw_method=bandwidth)\n","#           kde2 = gaussian_kde(weights2[idx][0][0], bw_method=bandwidth)\n","\n","#           x_values = np.linspace(min(min(weights1[idx][0][0]), min(weights2[idx][0][0])),\n","#                                 max(max(weights1[idx][0][0]), max(weights2[idx][0][0])), 1000)\n","\n","#           pdf_values1 = kde1(x_values)\n","#           pdf_values2 = kde2(x_values)\n","\n","#           plt.plot(x_values, pdf_values1, label=f'Without Dropout', linestyle='-')\n","#           plt.plot(x_values, pdf_values2, label=f'With Dropout', linestyle='--')\n","\n","#           plt.xlabel('Weight')\n","#           plt.ylabel('Estimated Density')\n","#           plt.legend()\n","#           plt.show()\n","\n","# for epoch in range(num_epochs):\n","#   plot_kde2(weight_history[epoch], weight_history3[epoch], epoch)\n"],"metadata":{"id":"FmCzdHIsDUlU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import torch\n","\n","def convert_to_json_serializable(obj):\n","    if isinstance(obj, torch.Tensor):\n","        return obj.cpu().detach().numpy().tolist()\n","    elif isinstance(obj, np.ndarray):\n","        return obj.tolist()\n","    elif isinstance(obj, (int, float, str)):\n","        return obj\n","    elif isinstance(obj, list):\n","        return [convert_to_json_serializable(item) for item in obj]\n","    elif isinstance(obj, dict):\n","        return {key: convert_to_json_serializable(value) for key, value in obj.items()}\n","    else:\n","        return None  # Handle other types as needed\n","\n","# Convert weight_history to a JSON-serializable format\n","serializable_weight_history = convert_to_json_serializable(weight_history3)\n","\n","# Convert to JSON\n","json_data = json.dumps(serializable_weight_history)\n","\n","# Save and download the JSON file\n","with open(\"0.5_weight_history_dropout.json\", \"w\") as json_file:\n","    json_file.write(json_data)\n","\n","# Download the JSON file in Google Colab\n","from google.colab import files\n","files.download(\"0.5_weight_history_dropout.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"S5HyYBd__PEU","executionInfo":{"status":"ok","timestamp":1697265507181,"user_tz":-360,"elapsed":756,"user":{"displayName":"Zaed Khan","userId":"11956878547931761934"}},"outputId":"6aaf07f2-ba87-4471-b21f-001d5e3f59a9"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f0fbecb5-f945-43cf-acb0-247ad460de70\", \"0.5_weight_history_dropout.json\", 23835587)"]},"metadata":{}}]},{"cell_type":"code","source":["# Convert weight_history to a JSON-serializable format\n","serializable_weight_history = convert_to_json_serializable(bias_history3)\n","\n","# Convert to JSON\n","json_data = json.dumps(serializable_weight_history)\n","\n","# Save and download the JSON file\n","with open(\"0.5_bias_history_dropout.json\", \"w\") as json_file:\n","    json_file.write(json_data)\n","\n","# Download the JSON file in Google Colab\n","from google.colab import files\n","files.download(\"0.5_bias_history_dropout.json\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"5sFct1iKoxhr","executionInfo":{"status":"ok","timestamp":1697265507182,"user_tz":-360,"elapsed":29,"user":{"displayName":"Zaed Khan","userId":"11956878547931761934"}},"outputId":"8b861fb5-eb90-461d-bc4e-43f6d41719b7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_36f884d7-dffd-437c-9347-9cea1c606aae\", \"0.5_bias_history_dropout.json\", 57745)"]},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1Kt2mbZTbIDsTGSFTgb_3iBJ51ivxPGtH","timestamp":1695810265067},{"file_id":"1J1LpblMPc8vBhNvnfiSY8nPkFivv3t3e","timestamp":1695619924736},{"file_id":"18-81BSGranVH6llh2dw4BPueaBIEGvSW","timestamp":1695520845611},{"file_id":"15CO-kicSAxhSli_ct5bf0Pwomg6isg7t","timestamp":1694951278115}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}